Hadoop in layman's term:
Hadoop grew out of a paper on MapReduce and GFS from Google, with the majority of its development coming from the efforts of  Doug Cutting
and Yahoo in the mid 2000s. At that time, the only companies capable of working with data at that scale are theSilicon Valley giants, when
the rest of the world was still mostly dependent on SQL solutions In 2006, Hadoop was released as an
open source project through Apache, as a result, it grew incredibly quickly in terms of related projects, ecosystem, and adoption.
Hadoop is a cornerstone to commercial data offerings today, mainly due to its huge community and ecosystem, as well as the ease of adoption.
As a company, you can spend relatively little to store the same amount of data using Hadoop in comparison to traditional data warehouses.
Here at Vertafore, we found our cost of storage is about 1/5 per gigabyte after migrating to Hadoop than it was on a SQL data warehouse for
our particular use case. Hadoop is also virtually infinitely scaleable both in terms of storage and performance, so there is almost no big
concerns about future challenges when adopting Hadoop.

Many of these consumers are analysts, business intelligence, industrial engineers, etc. Hadoop is slightly different for tech companies.
Companies like LinkedIn, Netflix, etc also use hadoop for the following purposes:
To store and process data in the terabytes/petabytes range. However, this data is often then re-consumed by other processes to build 
software features, such as Netflix’s recommendation system, or LinkedIn’s “People you may know” feature. From my experience, the two most 
popular use cases are for Business Analytics (tracking historical data to determine customer behavior to improve business processes), and 
as a distributed engine to process large amounts of data, often for tech companies (i.e. ingesting data from dozens of products in 
real-time, then using that to feed other software features). There’s no reason to limit Hadoop to these two cases, as it has found adoption
in things like modelling stock markets, parallel computing, distributed file storage, and so much more. This gave rise to a lot of 
complementary projects in the ecosystem like Spark, HBase, Kafka/Storm, etc.
As for Amazon, Facebook, Google, etc., they’re not exactly using Hadoop like most of the world. Although there are many processes that run
on MapReduce/HDFS inside those organizations, Amazon/FB/Google also has a lot of proprietary and cutting-edge technologies that has 
supplemented or replaced many components of Hadoop. For instance, Facebook uses Presto/RocksDB, Google has GFS/Beam. The idea is still the 
same: within your products, track everything, so that you can determine user behavior. For someone like Google/Facebook/Amazon, they know
what pages you like to view, what products you like to by, what people you like to follow. All of this is critical to building products 
that you as a consumer like to use. Google and Facebook also offer Hadoop or Hadoop-alternatives as an infrastructure-as-a-service model
(AWS, Google Data Platform), but that’s slightly unrelated to the question you’re asking, I think.
What I described above is only the tip of the iceberg, these days data has moved away from being reliant on Hadoop. It’s better to think
of it as a tool, as Hadoop =/= Big Data anymore. It simply is the most popular offering for any company to begin diving into Big Data.
Remember, knowledge is power, and in 2016, data is knowledge. Take everything I wrote with a grain of salt, this is simply my perception
of the current state of the world.
